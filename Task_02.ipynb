{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/flosch9/deep_learning_home_exam/blob/main/Task_02.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Load modules"
      ],
      "metadata": {
        "id": "Nt3-MycpxKDk"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "pip install utils"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nj16FXLexE_h",
        "outputId": "c80f4176-507a-4ee4-bb79-22951faac8a0"
      },
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting utils\n",
            "  Downloading utils-1.0.2.tar.gz (13 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: utils\n",
            "  Building wheel for utils (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for utils: filename=utils-1.0.2-py2.py3-none-any.whl size=13906 sha256=976e92dcc0aec7e191dfd177648dcecea7997a32a00983bdec6fb102c562b17c\n",
            "  Stored in directory: /root/.cache/pip/wheels/b8/39/f5/9d0ca31dba85773ececf0a7f5469f18810e1c8a8ed9da28ca7\n",
            "Successfully built utils\n",
            "Installing collected packages: utils\n",
            "Successfully installed utils-1.0.2\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define and load model (model.py)\n",
        "\n",
        "```\n",
        "# Als Code formatiert\n",
        "```\n",
        "\n"
      ],
      "metadata": {
        "id": "dfS5ObhvvHNx"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import utils\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.models import vgg16, VGG16_Weights\n",
        "\n",
        "\n",
        "class FCN32(nn.Module):\n",
        "    def __init__(self, output_size = (128,128), num_classes = 2):\n",
        "        super(FCN32, self).__init__()\n",
        "\n",
        "        # first part is regualr vgg16 (with batch normalization)?\n",
        "        self.features = vgg16(weights = VGG16_Weights.IMAGENET1K_V1).features#[0:28]\n",
        "\n",
        "        # only choose some parts of vgg16? [0:28]\n",
        "\n",
        "        # set ceil mode to true\n",
        "        #self.features[6].ceil_mode = True\n",
        "        #self.features[13].ceil_mode = True\n",
        "        #self.features[23].ceil_mode = True\n",
        "        #self.features[33].ceil_mode = True\n",
        "        #self.features[43].ceil_mode = True\n",
        "\n",
        "        # classifier is now replaced with another cnn (instead of a fc)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Conv2d(512, 4024, kernel_size=(3,3), stride=(1,1), padding=(1,1)), # 512 output from last cnn/maxpool layer #maybe only 1000 channels\n",
        "            # use filter size 1024 or 4096\n",
        "            nn.ReLU(True),\n",
        "            #with Relu? with Batchnorm? with maxpool?\n",
        "            # 7x7 filter\n",
        "            nn.Conv2d(4024, num_classes, kernel_size=1, stride=(1,1), padding=(1,1))\n",
        "            #nn.ReLU(True)\n",
        "\n",
        "            #nn.Softmax()\n",
        "            #softmax produces niceer output in the end\n",
        "\n",
        "            # makes difference in the output, and in the loss which (none) activation is used\n",
        "        )\n",
        "        self.upsample = nn.Sequential(\n",
        "            # what is with upsampling meant? this (just resizing) or the deconvolution before upsample and transposeconv2d the same???\n",
        "            # this one is not trainable but easier\n",
        "            nn.UpsamplingBilinear2d(size=(output_size)),\n",
        "            nn.Softmax()\n",
        "\n",
        "            #nn.ConvTranspose2d(num_classes, num_classes, kernel_size=4, stride=2, padding=0)#, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)\n",
        "            # then output needs to be adjusted and classes in one channel\n",
        "            # try to set ceiling of maxpool to true\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        #print(\"Output after classifier\")\n",
        "        #print(x.shape)\n",
        "        x = self.upsample(x)\n",
        "        #print(\"Output after upsampling\")\n",
        "        #print(x.shape)\n",
        "        return x\n",
        "\n",
        "#model = FCN32(output_size=(128,128))\n",
        "\n",
        "# print outputs of a model\n",
        "#print(vgg16_bn().features)\n",
        "#print(vgg16_bn().classifier)\n",
        "\n",
        "#print(vgg16_bn())\n",
        "#model = FCN32()\n",
        "\n",
        "# print(FCN32().features)\n",
        "# set ceil_modes to true\n",
        "#print(model.features[6].ceil_mode)\n",
        "\n"
      ],
      "metadata": {
        "id": "r-J0ghhDvoCf"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Define test functions (test.py)"
      ],
      "metadata": {
        "id": "bp7kvPYnv3Tl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import csv\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.datasets import OxfordIIITPet\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import PILToTensor, ToTensor, ToPILImage, Resize\n",
        "\n",
        "from model import FCN32\n",
        "\n",
        "\n",
        "###############################################################################\n",
        "# load model and parameters from training for testing\n",
        "###############################################################################\n",
        "\n",
        "# for loading model with trained parameters\n",
        "name_model = \"home_exam\\data\\problem2\\model_data\\\\\" + \"FCN32_version_01\"\n",
        "\n",
        "batch_size = 32\n",
        "resize_x = 128 #or256 or whatever\n",
        "resize_y = 128\n",
        "\n",
        "\n",
        "\n",
        "#model = FCN32()\n",
        "#model.load_state_dict(torch.load(name_model + \"_trained\"))\n",
        "\n",
        "###############################################################################\n",
        "# load dataset for testing\n",
        "###############################################################################\n",
        "\n",
        "# open test data, has 3669 samples\n",
        "# transform images (input images and lapels / mregion maps)to same sizes\n",
        "pets_test = OxfordIIITPet(root=\"home_exam\\data\\problem2\", split=\"test\", transform = Resize((resize_x,resize_y)), target_transform = Resize((resize_x,resize_y)), target_types=\"segmentation\", download=False)\n",
        "#print(len(pets_test))\n",
        "\n",
        "# do not use ToTensor for target image since it destroys the class\n",
        "def custom_collate(batch):\n",
        "\n",
        "    images = []\n",
        "    labels = []\n",
        "    for dataset in batch:\n",
        "        image = ToTensor()(dataset[0])\n",
        "        label = PILToTensor()(dataset[1]) # important else target fckd up with classes\n",
        "        label = label.view(resize_x, resize_y) #to get rid of the (implizit) given channel\n",
        "\n",
        "        label = label.long() # also importantz for CE-Loss, excpects long\n",
        "        label = torch.sub(label, 1) # also important since 3 classes -> [0,3), but original it was [1,3]\n",
        "\n",
        "        images.append(image)\n",
        "        labels.append(label)\n",
        "\n",
        "    return(torch.stack(images), torch.stack(labels))\n",
        "\n",
        "# load data for testing, use custom collate function\n",
        "# to handle non-tensor format of original dataset\n",
        "test_dataload = DataLoader(pets_test, batch_size=batch_size, shuffle=True, collate_fn= custom_collate)\n",
        "\n",
        "\n",
        "# short function for displaying initial image next to segmentation\n",
        "def display_data(data_point):\n",
        "\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize =(10,6))\n",
        "    ax[0].imshow(ToPILImage()(data_point[0][0]))\n",
        "    ax[1].imshow(ToPILImage()(data_point[1][0]))\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "    return()\n",
        "\n",
        "###############################################################################\n",
        "# testing model and displaying outputs\n",
        "###############################################################################\n",
        "\n",
        "def show_output(initial_image, model_output):\n",
        "\n",
        "    #print(\"Shape initial: {}\".format(initial_image.shape)) #(3, H,W)\n",
        "    #print(\"Shape segmentation: {}\".format(model_output.shape)) #(2,H,W)\n",
        "    #print(\"Unique values segmentation: {}\".format(torch.unique(model_output)))\n",
        "    #print(\"Output segmentation:\")\n",
        "    #print(model_output)\n",
        "\n",
        "    model_output = model_output[1,:,:]\n",
        "\n",
        "    fig, ax = plt.subplots(nrows=1, ncols=2, figsize =(10,6))\n",
        "    ax[0].imshow(ToPILImage()(initial_image))\n",
        "    ax[1].imshow(ToPILImage()(model_output), cmap = \"gray\")\n",
        "    fig.tight_layout()\n",
        "    plt.show()\n",
        "\n",
        "\n",
        "\n",
        "    return()\n",
        "\n",
        "\n",
        "def show_uncertainity_map():\n",
        "    return()\n",
        "\n",
        "def test_model(model, test_images, test_labels_merched, original_labels):\n",
        "\n",
        "    model.eval()\n",
        "\n",
        "    output = model(test_images)\n",
        "\n",
        "    show_output(test_images[0], output[0])\n",
        "\n",
        "    return()\n",
        "\n"
      ],
      "metadata": {
        "id": "1ij5b-Qhv7P2"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "eXqMHU6TvE_H"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Train model (train.py)"
      ],
      "metadata": {
        "id": "czdp_KzuvtfE"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import utils\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision.models import vgg16, VGG16_Weights\n",
        "\n",
        "\n",
        "class FCN32(nn.Module):\n",
        "    def __init__(self, output_size = (128,128), num_classes = 2):\n",
        "        super(FCN32, self).__init__()\n",
        "\n",
        "        # first part is regualr vgg16 (with batch normalization)?\n",
        "        self.features = vgg16(weights = VGG16_Weights.IMAGENET1K_V1).features#[0:28]\n",
        "\n",
        "        # only choose some parts of vgg16? [0:28]\n",
        "\n",
        "        # set ceil mode to true\n",
        "        #self.features[6].ceil_mode = True\n",
        "        #self.features[13].ceil_mode = True\n",
        "        #self.features[23].ceil_mode = True\n",
        "        #self.features[33].ceil_mode = True\n",
        "        #self.features[43].ceil_mode = True\n",
        "\n",
        "        # classifier is now replaced with another cnn (instead of a fc)\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Conv2d(512, 4024, kernel_size=(3,3), stride=(1,1), padding=(1,1)), # 512 output from last cnn/maxpool layer #maybe only 1000 channels\n",
        "            # use filter size 1024 or 4096\n",
        "            nn.ReLU(True),\n",
        "            #with Relu? with Batchnorm? with maxpool?\n",
        "            # 7x7 filter\n",
        "            nn.Conv2d(4024, num_classes, kernel_size=1, stride=(1,1), padding=(1,1))\n",
        "            #nn.ReLU(True)\n",
        "\n",
        "            #nn.Softmax()\n",
        "            #softmax produces niceer output in the end\n",
        "\n",
        "            # makes difference in the output, and in the loss which (none) activation is used\n",
        "        )\n",
        "        self.upsample = nn.Sequential(\n",
        "            # what is with upsampling meant? this (just resizing) or the deconvolution before upsample and transposeconv2d the same???\n",
        "            # this one is not trainable but easier\n",
        "            nn.UpsamplingBilinear2d(size=(output_size)),\n",
        "            nn.Softmax()\n",
        "\n",
        "            #nn.ConvTranspose2d(num_classes, num_classes, kernel_size=4, stride=2, padding=0)#, output_padding=0, groups=1, bias=True, dilation=1, padding_mode='zeros', device=None, dtype=None)\n",
        "            # then output needs to be adjusted and classes in one channel\n",
        "            # try to set ceiling of maxpool to true\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.features(x)\n",
        "        x = self.classifier(x)\n",
        "        #print(\"Output after classifier\")\n",
        "        #print(x.shape)\n",
        "        x = self.upsample(x)\n",
        "        #print(\"Output after upsampling\")\n",
        "        #print(x.shape)\n",
        "        return x\n",
        "\n",
        "#model = FCN32(output_size=(128,128))\n",
        "\n",
        "# print outputs of a model\n",
        "#print(vgg16_bn().features)\n",
        "#print(vgg16_bn().classifier)\n",
        "\n",
        "#print(vgg16_bn())\n",
        "#model = FCN32()\n",
        "\n",
        "# print(FCN32().features)\n",
        "# set ceil_modes to true\n",
        "#print(model.features[6].ceil_mode)\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "Qi1vPJ0eu_iz"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": [],
      "gpuType": "T4",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "accelerator": "GPU"
  },
  "nbformat": 4,
  "nbformat_minor": 0
}